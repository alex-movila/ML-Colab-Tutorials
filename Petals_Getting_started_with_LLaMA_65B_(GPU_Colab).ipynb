{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-movila/ML-Colab-Tutorials/blob/master/Petals_Getting_started_with_LLaMA_65B_(GPU_Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67\" width=\"40%\">  \n",
        "</div>\n",
        "\n",
        "# Getting started with Petals\n",
        "\n",
        "This notebook will guide you through the basics of Petals &mdash; a system for inference and fine-tuning 100B+ language models without the need to have high-end GPUs. With Petals, you can join compute resources with other people over the Internet and run large language models such as [LLaMA-65B](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md), [BLOOM-176B](https://huggingface.co/bigscience/bloom) or [BLOOMZ-176B](https://huggingface.co/bigscience/bloomz) from your desktop computer or Google Colab.\n",
        "\n",
        "üí¨ If you meet any issues while running this notebook, let us know in the **[#running-a-client](https://discord.gg/J29mCBNBvm)** channel of our Discord!\n",
        "\n",
        "So, let's get started! First, let's install [the Petals package](https://github.com/bigscience-workshop/petals):"
      ],
      "metadata": {
        "id": "VsXHWJLuowcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBC52TF3LVY1"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/bigscience-workshop/petals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. The easiest way to generate text üöÄ\n",
        "\n",
        "Let's start with the easiest task &mdash; creating a distributed model and using it for generating text. This machine will download a small part of the model weights and rely on other computers in the network to run the rest of the model.\n",
        "\n",
        "We suggest to start with LLaMA-65B, but you can also use BLOOM and BLOOMZ. Just set `MODEL_NAME = \"bigscience/bloom\"` or `\"biscience/bloomz\"` to load these models.\n",
        "\n",
        "üìã **Heads up:** This Colab is provided for demonstration purposes. If you build your own app running these models, make sure you follow the [LLaMA's](https://bit.ly/llama-license) and/or [BLOOM's](https://bit.ly/bloom-license) terms of use. Note that LLaMA is available for non-commercial purposes only, and you have to file a request [here](https://bit.ly/llama-license) to use it in your own projects."
      ],
      "metadata": {
        "id": "yEbot-oEXdpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uuX1IMLLotQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from petals import AutoDistributedModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"enoch/llama-65b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, add_bos_token=False)\n",
        "\n",
        "model = AutoDistributedModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to generate something by calling __`model.generate()`__ method.\n",
        "\n",
        "The first call to this method takes a few seconds to connect to the Petals swarm. Once we do that, you should expect generation speed of 3-5 tokens/sec for LLaMA-65B and ~1 tokens/sec for BLOOM. If you don't have enough GPUs to host the entire model, this is much faster than what you get with other methods, such as offloading, which gives 10&ndash;20 sec/token for BLOOM."
      ],
      "metadata": {
        "id": "zhyUxv13sfKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('A cat in French is \"', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s1IrE1H8wwr",
        "outputId": "1e40b217-0ae3-47e4-b893-d56ed8be1f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A cat in French is \"chat\" and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `model.generate()` method runs **greedy** generation by default. However, you can choose other generation methods like **top-p/top-k sampling** or **beam search** by passing the corresponding parameters (you'll see an example in a bit). You can even implement custom generation methods (we'll cover that in **Step 5**).\n",
        "\n",
        "üîè **Note:** Your data is processed by other people in the public swarm. Learn more about privacy [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety). For sensitive data, you can set up a [private swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm) among people you trust."
      ],
      "metadata": {
        "id": "02d0BDEAuUFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Chat bots and interactive generation üí¨\n",
        "\n",
        "If you'd like to talk to the model in an interactive way, you can use the __inference session__ interface. This interface provides a simple way to print generated tokens on the fly or make a chat bot that responds to human's phrases.\n",
        "\n",
        "The inference session looks for a sequence of servers that will run successive inference steps and store past attention caches. This way, you don't need to rerun previous tokens through the transformer to generate each phrase. If one of the remote servers fails, Petals will automatically find a replacement and regenerate a small part of the caches.\n",
        "\n",
        "Let's see how to use it to write a simple chat bot, showing tokens as soon as they are generated:"
      ],
      "metadata": {
        "id": "fZlzYVn0ApyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_token = tokenizer(\"^\")[\"input_ids\"][0]  # Workaround to make SentencePiece .decode() keep leading spaces\n",
        "\n",
        "with model.inference_session(max_length=512) as sess:\n",
        "    while True:\n",
        "        prompt = input('Human: ')\n",
        "        if prompt == \"\":\n",
        "            break\n",
        "        prefix = f\"Human: {prompt}\\nFriendly AI:\"\n",
        "        prefix = tokenizer(prefix, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "        print(\"Friendly AI:\", end=\"\", flush=True)\n",
        "\n",
        "        while True:\n",
        "            outputs = model.generate(\n",
        "                prefix, max_new_tokens=1, do_sample=True, top_p=0.9, temperature=0.75, session=sess\n",
        "            )\n",
        "            outputs = tokenizer.decode([fake_token, outputs[0, -1].item()])[1:]\n",
        "            print(outputs, end=\"\", flush=True)\n",
        "            if \"\\n\" in outputs:\n",
        "                break\n",
        "            prefix = None  # Prefix is passed only for the 1st token of the bot's response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5bdMaYYAqYR",
        "outputId": "8ece4869-da31-45ae-b3f7-daf7d69c253c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi, how are you?\n",
            "Friendly AI: I'm fine, thanks. And you?\n",
            "Human: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Making apps that use Petals\n",
        "\n",
        "If you develop a tool for other people, you can wrap up the code using Petals into a user-friendly web app, such as [chat.petals.ml](http://chat.petals.ml). Under the hood, this app may connect to a lightweight [HTTP endpoint](https://github.com/borzunov/petals-chat) for inference that forwards all requests to the Petals swarm.\n",
        "\n",
        "<div align=\"center\">\n",
        "<br>\n",
        "<img src=\"https://i.imgur.com/p2nwiho.png\" width=\"40%\">  \n",
        "</div>"
      ],
      "metadata": {
        "id": "P0k_0PrTAr6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. How does it work? üõ†Ô∏è\n",
        "\n",
        "The `model` you are running is equal to the original model, but only a part of it is loaded into your machine's GPU. Let's have a look under the hood:"
      ],
      "metadata": {
        "id": "557VBCGS8Dpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB-lCjKs8NEt",
        "outputId": "68be5b2d-05db-41d9-a9b9-d98826a13518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistributedLlamaForCausalLM(\n",
              "  (model): DistributedLlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
              "    (layers): RemoteSequential(modules=llama-65b-hf.0..llama-65b-hf.79)\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): LMHead()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, word embeddings and some other layers are regular PyTorch modules hosted on your machine, but the rest of the model (e.g., transformers blocks) is encased in the __RemoteSequential__ class. This is an advanced PyTorch module that runs on a distributed swarm of other machines.\n",
        "\n",
        "Still, you can access individual layers and their outputs, as well as run forward/backward through them:"
      ],
      "metadata": {
        "id": "9EW3wBDO-aTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_five_layers = model.model.layers[0:5]\n",
        "first_five_layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHYihUHH-Zo6",
        "outputId": "1acfc043-50f8-464b-d5e4-d2ee8c8151df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RemoteSequential(modules=llama-65b-hf.0..llama-65b-hf.4)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_inputs = torch.randn(1, 3, model.config.hidden_size, dtype=torch.bfloat16, requires_grad=True)\n",
        "outputs = first_five_layers(dummy_inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilOwoxr47Sso",
        "outputId": "fda0e151-b6f8-4d3c-b11f-12319b38f532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2139,  0.4180, -2.1562,  ..., -0.2031, -0.6523,  1.9844],\n",
              "         [-1.8594,  0.7070,  0.9141,  ...,  1.2109,  1.4219, -0.6680],\n",
              "         [ 1.0469,  0.0845, -0.9609,  ...,  1.6406,  1.3672,  1.1875]]],\n",
              "       dtype=torch.bfloat16, grad_fn=<_RemoteSequentialAutogradFunctionBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.mean((outputs - torch.ones_like(outputs)) ** 2)\n",
        "loss.backward()  # backpropagate through the internet\n",
        "print(\"Grad w.r.t. inputs:\", dummy_inputs.grad.flatten())"
      ],
      "metadata": {
        "id": "4XM6In8uArE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcbf8e1-cd51-4f18-e04e-677fd7f9004e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad w.r.t. inputs: tensor([-2.2507e-04, -1.9550e-05, -4.4250e-04,  ...,  5.8174e-05,\n",
            "         5.0545e-05,  5.9605e-05], dtype=torch.bfloat16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, you can mix and match distributed layers like in regular PyTorch and even insert and train your own layers (e.g., adapters) between the pre-trained ones."
      ],
      "metadata": {
        "id": "DxWOzWCwat6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://camo.githubusercontent.com/58732a64488a9be928e25f3e60e3692b989ffe212ac86cb4902d8df20a042b03/68747470733a2f2f692e696d6775722e636f6d2f525459463379572e706e67\" width=\"80%\">\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">üìú <b><a href=\"https://arxiv.org/pdf/2209.01188.pdf\">Read details in our paper</a></b></p>"
      ],
      "metadata": {
        "id": "0OZ5eXFrkfzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Adding a trainable adapter üèãÔ∏è\n",
        "\n",
        "While the remotely hosted transformer blocks are **frozen** to keep the pretrained model the same for all users, using **parameter-efficient adapters** (small trainable layers added between the pretrained blocks of the model, such as [LoRA](https://arxiv.org/abs/2106.09685)) or **trainable prompts** (trainable inputs added before the inputs to the model, such as in [P-Tuning v2](https://arxiv.org/abs/2110.07602)) is usually enough to make BLOOM solve a variety of downstream tasks.\n",
        "\n",
        "Below, we show an example of how to add a basic **trainable** linear layer between 5th and 6th transformer blocks of the pretrained model. The layer's weights and the corresponding optimizer statistics will be stored locally:"
      ],
      "metadata": {
        "id": "lJt4OS2pIZe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BloomBasedClassifier(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.distributed_layers = model.transformer.h\n",
        "    self.adapter = nn.Sequential(nn.Linear(model.config.hidden_size, 32), nn.Linear(32, model.config.hidden_size))\n",
        "    self.head = nn.Linear(model.config.hidden_size, 2)\n",
        "\n",
        "  def forward(self, embeddings):\n",
        "    mid_block = len(self.distributed_layers) // 2\n",
        "    hidden_states = self.distributed_layers[:mid_block](embeddings)\n",
        "    hidden_states = self.adapter(hidden_states)\n",
        "    hidden_states = self.distributed_layers[mid_block:](hidden_states)\n",
        "    pooled_states = torch.mean(hidden_states, dim=1)\n",
        "    return self.head(pooled_states)"
      ],
      "metadata": {
        "id": "YHPFitSoIZO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = BloomBasedClassifier(model).cuda()\n",
        "opt = torch.optim.Adam(classifier.parameters(), 3e-5)\n",
        "inputs = torch.randn(3, 2, model.config.hidden_size, device='cuda')\n",
        "labels = torch.tensor([1, 0, 1], device='cuda')\n",
        "\n",
        "for i in range(5):\n",
        "  loss = F.cross_entropy(classifier(inputs), labels)\n",
        "  print(f\"loss[{i}] = {loss.item():.3f}\")\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "print('predicted:', classifier(inputs).argmax(-1))  # l, o, l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ7AFOumIYlY",
        "outputId": "dffa3325-fd0c-4a5d-c9e4-d1c1ddabfd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss[0] = 11.039\n",
            "loss[1] = 6.550\n",
            "loss[2] = 2.489\n",
            "loss[3] = 0.455\n",
            "loss[4] = 0.038\n",
            "predicted: tensor([1, 0, 1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Using custom sampling methods üé∞\n",
        "\n",
        "The __`model.inference_session()`__ interface in Petals also allows you to write custom inference code. You can use this to implement any sampling algorithms you want, or write a custom beam search algorithm that forbids the model from using swearwords.\n",
        "\n",
        "Below, let's see how we can reimplement the standard `model.generate()` interface by making forward passes through all the layers manually:"
      ],
      "metadata": {
        "id": "xz6EJ8VsYW2b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frLiu0yiL6Sn",
        "outputId": "4064d246-f2f4-4c4b-e074-3fab62ab194c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jun 24 02:59:32.990 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A\n",
            "Jun 24 02:59:34.073 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chat\n",
            "Jun 24 02:59:35.088 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot\n",
            "Jun 24 02:59:36.103 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that\n",
            "Jun 24 02:59:37.165 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is\n",
            "Jun 24 02:59:38.200 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able\n",
            "Jun 24 02:59:39.219 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to\n",
            "Jun 24 02:59:40.245 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand\n",
            "Jun 24 02:59:41.226 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the\n",
            "Jun 24 02:59:42.229 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user\n",
            "Jun 24 02:59:43.208 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user‚Äô\n",
            "Jun 24 02:59:44.250 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user‚Äôs\n",
            "Jun 24 02:59:45.249 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user‚Äôs intent\n",
            "Jun 24 02:59:46.213 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user‚Äôs intent and\n",
            "Jun 24 02:59:47.206 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user‚Äôs intent and provide\n",
            "Jun 24 02:59:48.198 [\u001b[1m\u001b[34mINFO\u001b[0m] What is a good chatbot? Answer: A chatbot that is able to understand the user‚Äôs intent and provide the\n"
          ]
        }
      ],
      "source": [
        "from hivemind import get_logger\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "fake_token = tokenizer(\"^\")[\"input_ids\"][0]  # Workaround to make SentencePiece .decode() keep leading spaces\n",
        "\n",
        "text = \"What is a good chatbot? Answer:\"\n",
        "token_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "max_length = 100\n",
        "with torch.inference_mode():\n",
        "    with model.inference_session(max_length=max_length) as sess:\n",
        "        while len(text) < max_length:\n",
        "            embs = model.transformer.word_embeddings(token_ids)\n",
        "            embs = model.transformer.word_embeddings_layernorm(embs)\n",
        "\n",
        "            h = sess.step(embs)\n",
        "            h_last = model.transformer.ln_f(h[:, -1])\n",
        "            logits = model.lm_head(h_last)\n",
        "\n",
        "            next_token = logits.argmax(dim=-1)\n",
        "            text += tokenizer.decode([fake_token, next_token.item()])[1:]\n",
        "            token_ids = next_token.reshape(1, 1)\n",
        "            logger.info(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6. Making fox innocent ü¶ä\n",
        "\n",
        "Next, let's see how to fine-tune a model using trainable (optionally, deep) prompts.\n",
        "\n",
        "We'll take the model saying \"*A quick brown fox jumps over the lazy dog.*\" and teach it to say the opposite &ndash; that actually \"*A quick brown fox did not jump over the lazy dog*\"."
      ],
      "metadata": {
        "id": "miR8kS64ygvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"A quick brown fox\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=7)\n",
        "print(\"generated:\", tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHuVt22DzVvV",
        "outputId": "8c98885f-fc35-4d5e-877a-b684a4399434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated: A quick brown fox jumps over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoDistributedModelForCausalLM.from_pretrained(MODEL_NAME, tuning_mode='deep_ptune', pre_seq_len=3)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "qLsi50_l9UMS",
        "outputId": "a49f36ec-6168-494e-a8a9-a2e8b34ae8f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jun 23 15:12:42.335 [\u001b[1m\u001b[34mINFO\u001b[0m] Using DHT prefix: llama-65b-hf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "the_fox_is_innocent = tokenizer(\"A quick brown fox did not jump over the lazy dog\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "for i in range(30):\n",
        "    loss = model(input_ids=the_fox_is_innocent, labels=the_fox_is_innocent).loss\n",
        "    print(f\"loss[{i}] = {loss.item():.3f}\")\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(\"opt.step()\")"
      ],
      "metadata": {
        "id": "Zm2eWZew9b3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"A quick brown fox\", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=7)\n",
        "print(\"generated:\", tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "EZYRU7v_A5DO",
        "outputId": "24bc9dbb-7bd7-4290-e36c-6f6249e88073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated: A quick brown fox did not jump over the lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7. Sharing is caring ü§ó\n",
        "\n",
        "We developed Petals to be a community-run system, so we rely on people giving out their GPUs to increase the swarm‚Äôs capacity. If you have some GPUs that are not always busy, please **consider running a Petals server.** You can pause it any time if you want to use the GPUs for something else. As a bonus, people running a server get a certain speedup when using Petals, since a larger part of the model is hosted locally.\n",
        "\n",
        "<br>\n",
        "\n",
        "üêã You can run our [Docker](https://www.docker.com) image (works on Linux, macOS, and Windows with [WSL2](https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl)):\n",
        "\n",
        "```\n",
        "sudo docker run -p 31330:31330 --ipc host --gpus all --volume petals-cache:/cache --rm \\\n",
        "    learningathome/petals:main python -m petals.cli.run_server bigscience/bloom --port 31330\n",
        "```\n",
        "\n",
        "üêç Or run these commands in an [Anaconda](https://www.anaconda.com) env (requires Linux and Python 3.7+):\n",
        "\n",
        "```\n",
        "conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia\n",
        "pip install -U petals\n",
        "python -m petals.cli.run_server bigscience/bloom\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "üìö See [FAQ](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-server) to learn how to configure the server to use multiple GPUs, address common issues, etc.\n",
        "\n",
        "You can also host [BLOOMZ](https://huggingface.co/bigscience/bloomz), a version of BLOOM fine-tuned to follow human instructions in the zero-shot regime ‚Äî just replace `bloom-petals` with `bloomz-petals`.\n",
        "\n",
        "üîí Hosting a server does not allow others to run custom code on your computer. Learn more about security [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety).\n",
        "\n",
        "üí¨ If you have any issues or feedback, let us know on [our Discord server](https://discord.gg/D9MwApKgWa)!"
      ],
      "metadata": {
        "id": "iK_iT8J3zDC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7. Using other fine-tuning and prompt-tuning methods\n",
        "\n",
        "While you can write your own custom adapters, Petals implements several [standard](https://arxiv.org/abs/2104.08691) [methods](https://arxiv.org/abs/2101.00190) for parameter-efficient fine-tuning. We provide a couple of advanced examples in our GitHub repository:\n",
        "\n",
        "- Training a personified chatbot: [notebook](https://github.com/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb)\n",
        "\n",
        "- Fine-tuning BLOOM for text semantic classification: [notebook](https://github.com/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb)"
      ],
      "metadata": {
        "id": "fwLMNcJ80ARs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's next?\n",
        "\n",
        "Congratulations on finishing our tutorial! Now, you are familiar with how to use Petals for different tasks, how it works under the hood, and how to increase its capacity.\n",
        "\n",
        "You can find a few other helpful resources below:\n",
        "\n",
        "* __More about Petals.__ The [README](https://github.com/bigscience-workshop/petals#readme) file in our GitHub repository has links to more Petals-related materials, including instructions for starting your own swarm (possibly, with a model other than BLOOM).\n",
        "\n",
        "* __Discord server.__ If you have any feedback, questions, or technical issues, please [join our Discord server](https://discord.gg/D9MwApKgWa) and let us know. If you want to build something based on Petals, we'd be happy to hear what you are up to.\n",
        "\n",
        "* __Academic paper.__ We have released a [paper](https://arxiv.org/abs/2209.01188) that goes into details about our research and what happens in Petals under the hood."
      ],
      "metadata": {
        "id": "PGUpxhQxXVCF"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}